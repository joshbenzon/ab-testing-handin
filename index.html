<html lang="en">
    <head>
        <!-- Meta Information -->
        <meta charset="UTF-8" />
        <meta http-equiv="X-UA-Compatible" content="IE=edge" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0" />

        <!-- Font Styles -->
        <!-- DM SERIF Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&family=DM+Serif+Text:ital@0;1&display=swap"
            rel="stylesheet"
        />

        <!-- DM SANS Font -->
        <link rel="preconnect" href="https://fonts.googleapis.com" />
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
        <link
            href="https://fonts.googleapis.com/css2?family=Bree+Serif&family=DM+Sans:ital,opsz,wght@0,9..40,100..1000;1,9..40,100..1000&display=swap"
            rel="stylesheet"
        />

        <!-- Page Title -->
        <title>A/B Testing HandIn</title>

        <!-- CSS Styling -->
        <link rel="stylesheet" href="styles.css" />
    </head>

    <body>
        <!-- TITLE -->
        <section id="title">
            <h1>A/B Testing</h1>
            <p>
                Using statistical analysis to evaluate different design
                decisions
            </p>

            <img
                id="final-product"
                src="./images/A:B Testing.png"
                alt="A/B Testing"
            />
        </section>

        <!-- OVERVIEW -->
        <section>
            <h2>Overview</h2>
            <p>
                In this project, I redesigned an appointment scheduling system
                and gathered data between the original design and my new design.
                In order to evaluate which design is "better" according to our
                needs, I used hypothesis testing and statistical analysis.
            </p>
            <p>
                Additionally, the overarching question we are proving is how the
                organization of visual elements and the different colors both
                impact the response rate of certain tasks. Likewise, A/B Testing
                can help us make this observation by analyzing both speed and
                accuracy when performing tasks.
            </p>
        </section>

        <hr aria-label="outer-divider" class="outer-divider" />

        <!-- PART 1 -->
        <section>
            <h2>Part 1: Data Collection</h2>

            <section>
                <p>
                    In studio, we gather data from students from two different
                    designs: "Design A" resembled the original design and
                    "Design B" resembled my own redesign. We analyzed different
                    metrics to see which one is "better."
                </p>
                <p>
                    In regards to my design changes, I decided to "gray out" the
                    buttons that the appointment criteria did not aligned with
                    and also made the appointment that did align with green. I
                    also separated each appointment block with a divider to
                    differentiate the grouping of text and buttons. This allowed
                    users to pinpoint exactly, which appointments are available
                    given their criteria.
                </p>

                <div class="horizontal-flex">
                    <section>
                        <h3>Design A</h3>

                        <div class="image">
                            <img
                                src="./designs/Design A.png"
                                alt="Design A (Original)"
                            />
                            <p class="caption">Design A (Original)</p>
                        </div>
                    </section>

                    <hr
                        aria-label="inner-divider"
                        class="inner-divider vertical-divider"
                    />

                    <section>
                        <h3>Design B</h3>
                        <div class="image">
                            <img
                                src="./designs/Design B.png"
                                alt="Design B (Redesign)"
                            />
                            <p class="caption">Design B (Redesign)</p>
                        </div>
                    </section>
                </div>
            </section>
        </section>

        <hr aria-label="outer-divider" class="outer-divider" />

        <!-- PART 2 -->
        <section>
            <h2>Part 2: Analysis</h2>

            <section>
                <h3>Creating Hypotheses</h3>

                <h4>Misclick Rate</h4>
                <h5>
                    This metric of choice resembles a boolean flag indicating if
                    the user pushed a button external to the task.
                </h5>
                <p>
                    H<sub>0</sub>: There will be no difference in the misclick
                    rate between Design A and Design B.
                </p>
                <p>
                    H<sub>1</sub>: Version A will have a higher misclick rate
                    than Version B.
                </p>
                <ul>
                    <li>
                        Because Version A has all the buttons be the same color
                        blue, there's bound to be a higher number of misclicks,
                        since there's no differentiation between what's the
                        correct button. On the other hand, Version B has a clear
                        visual green color and space differentiation with line
                        dividers to prevent users from confusing which
                        appointment and which button to press.
                    </li>
                </ul>

                <h4>Time on Page</h4>
                <h5>
                    This metric of choice resembles the total time (in
                    milliseconds) that the user spent on the page.
                </h5>
                <p>
                    H<sub>0</sub>: There will be no difference in the time spent
                    on page between Design A and Design B.
                </p>
                <p>
                    H<sub>1</sub>: Version A will have a higher time spent on
                    the page than Version B.
                </p>
                <ul>
                    <li>
                        Because Version A has no differentiation between which
                        buttons go with which appointment slot, there's bound to
                        be more time spent on the page, as users have to read
                        each appointment description. On the other hand, Version
                        B has a clear visual green color and space
                        differentiation with line dividers to allow users to not
                        spend as much time reading appointment descriptions.
                    </li>
                </ul>

                <h4>Number of Clicks</h4>
                <h5>
                    My metric of choice resembles the number of times the user
                    clicked on the screen.
                </h5>
                <p>
                    H<sub>0</sub>: There will be no difference in the number of
                    clicks between Design A and Design B.
                </p>
                <p>
                    H<sub>1</sub>: Version A will have a higher number of clicks
                    than Version B.
                </p>
                <ul>
                    <li>
                        Because Version A has the same color and opacity levels
                        for all the buttons, there's bound to be more clicks, as
                        any button is deemed available to click from. On the
                        other hand, Version B has unavailable buttons to be
                        completely grayed out with a slight decrease in opacity
                        in order to prevent users from clicking the incorrect
                        buttons. In contrast, the correct buttons are bold and
                        visually stand out from the rest.
                    </li>
                </ul>

                <p class="extra-text">
                    Additionally, I predict for all 3 of these metrics that we
                    will reject each of the null hypothesis. Design B offers
                    both an organized differentiation with line dividers between
                    each appointment block and also a clear visual change for
                    appointments that fit the user's needs and wants (gray vs.
                    green buttons). Hence, the Misclick Rate, Time on Page, and
                    the Number of Clicks will all be lower for Design B compared
                    to Design A.
                </p>
            </section>

            <hr aria-label="inner-divider" class="inner-divider" />

            <section>
                <h3>Run Statistical Tests on the Data</h3>
                <h4>Misclick Rate</h4>
                <div class="image">
                    <img
                        src="./results/Result (Misclick Rate).png"
                        alt="Results of Results (Misclick Rate)"
                    />
                    <p class="caption">Summary of Results (Misclick Rate)</p>
                </div>

                <p>
                    The test we used is the "One-Tailed T-Test," because it
                    compares an experimental value (from Design B) that's either
                    bigger or smaller than a baseline number (from Design A).
                    The p-value represents the chance that the groups are
                    actually the same.
                </p>
                <p>
                    Since we hypothesized that Version A's value is larger than
                    Version B, we need to take the compliment of the p-value: 1
                    - 0.9969066735 = 0.0030933265. Since the p-value of
                    0.0030933265 is less than 0.05, we reject the null
                    hypothesis (as the low p-value signals that the groups
                    aren't likely the same). Thus, we found statistically
                    significant evidence that the alternative hypothesis is
                    true.
                </p>
                <p>
                    Likewise in regards to the misclick rate, the average value
                    of Design A is about a 0.206 rating and for Design B is a 0
                    rating. Hence, since a positive value resembles "True," we
                    are confident that the hypothesis of "Version A will have a
                    higher misclick rate than Version B" is true.
                </p>

                <h4>Time on Page</h4>
                <div class="image">
                    <img
                        src="./results/Result (Time on Page).png"
                        alt="Results of Results (Time on Page)"
                    />
                    <p class="caption">Summary of Results (Time on Page)</p>
                </div>

                <p>
                    The test we used is the "One-Tailed T-Test," because it
                    compares an experimental value (from Design B) that's either
                    bigger or smaller than a baseline number (from Design A).
                    The p-value represents the chance that the groups are
                    actually the same.
                </p>
                <p>
                    Since we hypothesized that Version A's value is larger than
                    Version B, we need to take the compliment of the p-value: 1
                    - 0.9751880081 = 0.0248119919. Since the p-value of
                    0.0248119919 is less than 0.05, we reject the null
                    hypothesis (as the low p-value signals that the groups
                    aren't likely the same). Thus, we found statistically
                    significant evidence that the alternative hypothesis is
                    true.
                </p>
                <p>
                    Likewise in regards to the time on the page, the average
                    value of Design A is about 12860 milliseconds and for Design
                    B is about 8259 milliseconds. Hence, we are confident that
                    the hypothesis of "Version A will have a higher time spent
                    on the page than Version B" is true.
                </p>

                <h4>Number of Clicks</h4>
                <div class="image">
                    <img
                        src="./results/Result (Number of Clicks).png"
                        alt="Results of Results (Number of Clicks)"
                    />
                    <p class="caption">Summary of Results (Number of Clicks)</p>
                </div>

                <p>
                    The test we used is the "One-Tailed T-Test," because it
                    compares an experimental value (from Design B) that's either
                    bigger or smaller than a baseline number (from Design A).
                    The p-value represents the chance that the groups are
                    actually the same.
                </p>
                <p>
                    Since we hypothesized that Version A's value is larger than
                    Version B, we need to take the compliment of the p-value: 1
                    - 0.9331260592 = 0.0668739408. Since the p-value of
                    0.0248119919 is greater than 0.05, we fail to reject the
                    null hypothesis (as the higher p-value signals that the
                    groups are likely the same). Thus, we did NOT find
                    statistically significant evidence that the alternative
                    hypothesis is true.
                </p>
                <p>
                    Likewise in regards to the number of clicks, the average
                    value of Design A is about 3 clicks and for Design B is
                    about 2 clicks. Hence, we are NOT confident that the
                    hypothesis of "Version A will have a higher number of clicks
                    than Version B" is true.
                </p>
            </section>

            <hr aria-label="inner-divider" class="inner-divider" />

            <section>
                <h3>Summary Statistics</h3>
                <p>
                    When it comes to Design A, the average misclick rate is
                    about a rate of 0.206 (variance: 0.168, median: 1, mode: 0),
                    the average time on the page is about 12860 milliseconds
                    (variance: 140609949, median: 8223, mode: n/a), and the
                    average number of clicks is about 3 clicks (variance: 8.750,
                    median: 2, mode: 2).
                </p>
                <p>
                    When it comes to Design B, the average misclick rate is a
                    rate of 0 (variance: 0, median: 0, mode: 0), the average
                    time on the page is about 8259 milliseconds (variance:
                    33783195, median: 7401, mode: n/a), and the average number
                    of clicks is about 2 clicks (variance: 0.413, median: 2,
                    mode: 2).
                </p>
                <p>
                    Both Design A and Design B have above 30 data points and a
                    mostly equal distribution (39 vs. 36). Hence, we can see
                    that overall Design A had a higher misclick rate, a higher
                    time on the page, and higher number (average and median) of
                    clicks than Design B. Thus, this may be evidence to show
                    that the designs in Design B have better accuracy and faster
                    rates of performing the task.
                </p>
            </section>
        </section>

        <hr aria-label="outer-divider" class="outer-divider" />

        <!-- CONCLUSION -->
        <section id="conclusion">
            <h3>Conclusion</h3>
            <p>
                In conclusion, I found that it's difficult to pinpoint which
                designs are "better," as there's many different metrics to
                compare against. Finally, I found that utilizing statistical
                analysis with the scientific method is how we can mathematically
                prove which designs are indeed "better."
            </p>
        </section>
    </body>
</html>
